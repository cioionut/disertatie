\chapter{Sistem de dialog}

% todo: citari de sericii si lucrari

Construcția unui agent virtual menit să întrețină cursul unei conversații a fost întotdeauna un etalon al performanței de cercetare, de aceea testul ce poartă numele cercetătorului britanic Alan Turing \ref{test-turing} a fost pană de curând un criteriu în această direcție.

Au fost propuse diferite arhitecturi și moduri de a schița matematic o conversație, printre ele și ELIZA compus dintr-un set de reguli elaborate pe baza mai multor studii având la bază logica. Bineînțeles industria cere și abordări mai modulare, mai robuste dar care să nu iasă din tipare, în această speță făcându-și apariția prima arhitectură bazată pe umplerea de sloturi (GUS), adică pentru fiecare replică din dialog se extrag constituenți semantici specifici domeniului care mai apoi sunt completați într-o structură tabelară (frame) urmând să servească drept parametrii în interogările cu sistemul.

Dacă privim la scopul final al unei conversații distingem două clase și anume: agenți orientați pe rezolvarea de cerințe și agenți orientați pe discuție la nivel general.

Cum majoritatea asistenților virtuali sunt dezvoltați în scopuri comerciale, rezolvarea cerințelor primează în funcționalitatea unei astfel de aplicații, așadar industria se concentrează pe arhitecturi modulare, care să necesite cât mai puține date de antrenare, RASA, SNIPS, WATSON, DialogFlow sunt platforme care pun la dispoziție instrumente pentru a crea genul acesta de agenți.

Prin natura sa, lumea academică este întotdeauna mai în față în ceea ce privește tehnica sa de a concepe aceste sisteme, mai exact se folosesc rețele neurale recurente pentru a capta contextul unei replici și pentru a genera răspunsuri, iar pentru contextul conversației și generarea unor politici de răspuns se folosește învățarea prin recompensă. \cite{rl-seq2seq}

În cadrul studiului curent accentul este pus pe structura ce îmbină mai multe modele, drept pentru care vor fi detaliate experimentele pentru NLU și DM.

\section{Înțelegerea limbajului natural}
NLU este un concept destul de vast, in terminologia sistemelor de dialog acesta joacă rolul componentei care detectează intenția vorbitorului si extrage constituenți semantici din limbajul natural exprimat.
Detecția intenției poate fi tratata ca o problema de clasificare a unei replici din punct de vedere semantic, iar recunoașterea entităților poate fi văzută ca o etichetare de secvențe.
\subsection{Abordări anterioare}

% todo: de citat lucrarile

Pentru detectarea intenției se pot folosi tehnici standard de clasificare a unui text si anume SVM (Haffner et al., 2003), dar si rețele neurale convoluționale (CNNs) (Xu and Sarikaya, 2013) întâlnite adesea în procesarea imaginilor.

Pentru recunoașterea entităților se folosesc abordări precum (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), și rețele neurale recurente (RNNs) (Yao et al., 2014; Mesnil et al., 2015)

Având la baza înțelegerea limbajului, majoritatea lucrărilor actuale se concentrează pe rezolvarea concomitenta a acestor doua probleme, întrucât combinarea celor doua modele ajuta la învățarea unei reprezentări cat mai precise a textului. \cite{joint models-attention models}


\subsection{Model propus}

Rețelele neuronale recurente (RNN) sunt unele dintre cele mai populare modele matematice folosite în procesarea limbajului natural, avantajul pe care acestea îl oferă se datorează într-o mare măsură formelor sale LSTM, GRU, dar și prin caracterul lor de a învăța din date cu caracter secvențial. Aceste tipuri de rețele sunt folosite adesea ca structuri de bază pentru a crea alte modele de calcul, un exemplu este rețeaua de codificare-decodificare (seq2seq), formată din două rețele recurente, una pentru captarea (codarea) înțelesului unei secvențe iar cealaltă pentru operația de decodificare într-o secvență de ieșire.

Modelul propus pentru înțelegere a limbajului rezolvă ambele probleme simultan folosind un codificator pentru reprezentarea înțelesului unei propoziții și două decodificatoare câte unul pentru fiecare chestiune. Figura \ref{fig:seq2seq_x} sintetizează procesele implicate în acest model.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{seq2seq_x.png}
	\caption{Seq2Seq-x}
	\label{fig:seq2seq_x}
\end{figure}

Pentru recunoașterea entităților se dorește etichetarea unei secvențe de cuvinte $ x= (x_1, ..., x_T)$ într-o altă secvență de nume de entități corespunzătoare $ y=(y_1, ..., y_T) $. Operația de codificare se realizează prin citirea întregii secvențe de către o rețea recurentă bidirecțională, înțelesul secvenței de intrare este reprezentat de suma celor două stări ascunse finale, adică pentru citirea de la stânga la dreapta obținem stările ascunse $(fh_1, ..., fh_T)$ și invers $(bh_T, ..., bh_1)$, apoi suma celor două $h_i = [fh_i + bh_i]$ este folosită drept codificare a înțelesului secvenței de intrare. Alte lucrări \cite{joint_online_bing} folosesc ca stare ascunsă finală $h_i$ o concatenare a celor două stări. Alegerea de a aduna cele două reprezentări în schimbul concatenării, ține de faptul că dimensiunea reprezentării finale crește în cazul concatenării, fiind mai dificil de antrenat, dublând în acest caz și jumătate din parametrii rețelei. Figura \ref{fig:enc_module} descrie procesul de codificare.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{encoder_module.png}
	\caption{Bidirectional Encoder}
	\label{fig:enc_module}
\end{figure}

Vom folosi reprezentarea captată de codificator pentru a inițializa operația de decodificare, aceasta este produsă de către o rețea recurentă unidirecțională, văzută ca o funcție $s_i=f(s_{i-1}, y_{i-1}, h_i, c_i)$ care la fiecare pas $i$, calculează o nouă stare $s_i$, bazată pe: starea anterioară a decodificatorului $s_{i-1}$, eticheta anterior prezisă, $y_{i-1}$, starea ascunsă din codificator corespunzătoare pasului curent, $h_i$ și un vector de context $c_i$, calculat ca o sumă ponderată peste stările ascunse din codificator.
Inspirat din \cite{trans_luong_manning} am experimentat mai multe modele de atenție.
$$ c_i = \sum_{j=1}^{T} \alpha_{i,j} h_j$$
$$ \alpha_{i,j} = \frac{\exp(score_{i, j})}{\sum_{k=1}^{T} \exp(score_{i, k})}$$

\[ score(i, k) = g(s_{i-1}, h_k) =
\begin{cases}
s_{i-1}^\top h_k			&	\quad \text{dot}\\
s_{i-1}^\top W_a h_k	&	\quad \text{general}\\
v_a^\top \tanh(W_a [s_{i-1};h_k])	&	\quad \text{concat}\\
\end{cases}
\]

Modelul de atenție poate fi văzut ca o rețea de tip feed-forward $g(s_{i-1}, h_k)$, care la fiecare pas din operația de decodare calculează ponderile pentru stările ascunse ale codificatorului, folosindu-se de ultima stare ascunsă din decodor, $s_{i-1}$ și stările ascunse $(h_1, ..., h_T)$. Mai jos în figura \ref{fig:dec_bah} este descris întreg fluxul de date și operații realizate de decodorul responsabil de etichetarea entităților.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{decoder_bahdanau.png}
	\caption{Attention Decoder}
	\label{fig:dec_bah}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mulțimea de antrenare}
ATIS (Airline Travel Information Systems) \cite{atis} este o colecție de cereri venite din partea clienților referitor la zborurile aeriene de la începutul anilor 90. DARPA este instituția americana ce s-a ocupat de colectarea acestor date. Mulțimea de antrenare conține un număr de aproape 5000 de propoziții etichetate cu intenții si entități. Formatul de etichetare utilizat este IOB (Inside–outside–beginning), care ajuta la etichetarea entitatilor formate din mai multe cuvinte, astfel încât fiecare etichetă este precedata de B-<nume-eticheta> pentru început, iar cu I-<nume-eticheta> pentru următoarele cuvinte care fac parte din entitate. Exemplele sunt date în limba engleză, iar lungimea medie a unei prepoziții este de 15 cuvinte.

\begin{table}
	\centering
	\caption{Exemplu din mulțimea de antrenare}
	\label{atis_example}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
			\hline
			\textbf{Propoziție} & flights & from & New & York & to & Cicago & on & wednesday & morning \\ 
			\hline
			\textbf{Entități} & O & O & B-from.loc & I-from.loc & O & B-to.loc & O & B-depart.day-name & B-depart.day-mood \\
			\hline
			\textbf{Intenție} & \multicolumn{8}{c}{flight} &  \\ 
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}
	\centering
	\caption{Statistici ATIS}
	\label{atis_stats}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ |c|c|c|c|c|c|c| } 
			\hline
			\textbf{Multime de antrenare} & \#Train & \#Dev & \#Test & $|V|$ & \#Intenții & \#Entități \\ 
			\hline
			ATIS & 4978 & - & 893 & 900 & 18 & 80 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}




%%%%%%%%%%%%%%%%%%%%
\subsection{Rezultate}
\begin{center}
	\begin{tabular}{ c c c } 
		\hline
		\textbf{Model} 		 & \textbf{Scor F1 (entități)} & \textbf{Scor F1 (intenții)}\\
		\hline
		Seq2SeqX (dot - attn) & 97,5 & 96,5 \\
		\hline
		Seq2SeqX (general - attn) & 96.6 & 96,5 \\
		\hline
		Seq2SeqX (concat - attn) & 97.53 & - \\
		\hline
		Seq2SeqX (no - attn) & 97.8 & - \\
		\hline
		Seq2SeqX (no - attn, concat - attn) & 97.4 & 96.0 \\
		\hline
		Seq2SeqX (concat - intent attn) & 97.55 & 96.53 \\
		\hline
	\end{tabular}
\end{center}

\subsection{Analiza erorilor}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Administrator de dialog}

\subsection{Abordări anterioare}